import argparse
import os.path

from torch import optim
from data.load_data import *
from tools.utils import *
from transformers import BertTokenizer
from tensorboardX import SummaryWriter
from tools.train_api import single_epoch_train, evaluate_on_dataset
import time
import shutil
import pprint

from dataset.nr3d_dataset import make_data_loaders
from models.refer_net import ReferIt3DNet_transformer


def parse_arguments():
    parser = argparse.ArgumentParser(description='ReferIt3D training')

    parser.add_argument('-scannet-file', type=str, default='../scannet/scannet_00_views.pkl',
                        help='pkl file containing the data of Scannet generated by running ...')
    parser.add_argument('-refer_train_file', type=str, default='./data/referit3d/nr3d_train.csv')
    parser.add_argument('-refer_val_file', type=str, default='./data/referit3d/nr3d_test.csv')
    parser.add_argument('--project', type=str, default='Nr3D_SeCG')
    parser.add_argument('--res-dir', type=str, default='./runs',
                        help='where to save training-progress, model, etc')
    parser.add_argument('--log-iter', type=int, default=100)

    parser.add_argument('--min-word-freq', type=int, default=3)

    parser.add_argument('--n-workers', type=int, default=0, help='number of data loading workers')
    parser.add_argument('--init-lr', type=float, default=0.0005, help='learning rate for training.')

    parser.add_argument('--max-distractors', type=int, default=51,
                        help='Maximum number of distracting objects to be drawn from a scan.')
    parser.add_argument('--max-test-objects', type=int, default=88)
    parser.add_argument('--points-per-object', type=int, default=1024,
                        help='points sampled to make a point-cloud per object of a scan.')
    parser.add_argument('--random-seed', type=int, default=2020,
                        help='Control pseudo-randomness (net-wise, point-cloud sampling etc.) fostering reproducibility.')

    parser.add_argument('--device', type=str, default='0')
    parser.add_argument('--batch-size', type=int, default=2, help='batch size per gpu. [default: 32]')
    parser.add_argument('--max-epoch', type=int, default=100)

    parser.add_argument('--bert-pretrain-path', type=str, default='./pretrained/bert')
    parser.add_argument('--resume-path', type=str, default='', help='model-path to resume')
    parser.add_argument('--pn-path', type=str, default='./pretrained/ckpt_cls40.pth', help='pretrained clf model')

    parser.add_argument('--sem_encode', type=bool, default=True)
    parser.add_argument('--m', type=int, default=10, help='memory matrix height')
    parser.add_argument('--gat', type=bool, default=True)
    parser.add_argument('--lay-number', type=int, default=2)
    parser.add_argument('--multi_pos', type=bool, default=True)

    parser.add_argument('--view-number', type=int, default=4)
    parser.add_argument('--rotate-number', type=int, default=4)
    parser.add_argument('--aggregate-type', type=str, default='avg')
    parser.add_argument('--encoder-layer-num', type=int, default=3)
    parser.add_argument('--decoder-layer-num', type=int, default=4)
    parser.add_argument('--decoder-nhead-num', type=int, default=8)
    parser.add_argument('--object-latent-dim', type=int, default=768)
    parser.add_argument('--inner-dim', type=int, default=768)
    parser.add_argument('--dropout-rate', type=float, default=0.15)
    parser.add_argument('--lang-cls-alpha', type=float, default=0.5, help='if > 0 a loss for guessing the target via '
                                                                          'language only is added.')
    parser.add_argument('--obj-cls-alpha', type=float, default=0.5, help='if > 0 a loss for guessing for each segmented'
                                                                         ' object its class type is added.')

    args = parser.parse_args()

    args.tb_path = os.path.join(args.res_dir, 'logs', args.project)
    args.save_dir = os.path.join(args.res_dir, args.project)
    if args.resume_path is None:
        if os.path.exists(args.tb_path):
            shutil.rmtree(args.tb_path)
        if os.path.exists(args.save_dir):
            shutil.rmtree(args.save_dir)
    os.makedirs(args.save_dir, exist_ok=True)
    logger = create_logger(args.save_dir)
    args_string = pprint.pformat(vars(args))
    logger.info(args_string)
    return args, logger


if __name__ == '__main__':
    # Parse arguments
    args, logger = parse_arguments()
    all_scans_in_dict, class_to_idx, referit_data = load_filtered_data(args.scannet_file, args.refer_train_file,
                                                                       args.refer_val_file)
    # drop unused scans
    all_scans_in_dict = trim_scans_per_referit3d_data(referit_data, all_scans_in_dict)

    training_scan_ids = set(referit_data[referit_data['is_train']]['scan_id'])
    # test_scan_ids = set(referit_data[~referit_data['is_train']]['scan_id'])
    print('{} training scans will be used.'.format(len(training_scan_ids)))
    mean_rgb = mean_color(training_scan_ids, all_scans_in_dict)
    logger.info(mean_rgb)
    # mean_rgb = [[0.4585, 0.4149, 0.3644]]
    instance2semantic = list(all_scans_in_dict.values())[0].dataset.instance_cls2semantic_cls
    args.language_word_label = {}
    for k, v in instance2semantic.items():
        args.language_word_label[k] = v
        ss = k.strip().split(' ')
        if len(ss) > 1 and ss[-1] not in instance2semantic:
            args.language_word_label[ss[-1]] = v

    data_loaders = make_data_loaders(args, referit_data, class_to_idx, all_scans_in_dict, mean_rgb)

    if torch.cuda.is_available():
        device = torch.device('cuda')
        seed_training_code(args.random_seed)
    else:
        device = torch.device('cpu')

    class_name_list = []
    for cate in class_to_idx:
        class_name_list.append(cate)

    tokenizer = BertTokenizer.from_pretrained(args.bert_pretrain_path)

    n_classes = len(class_to_idx) - 1  # -1 to ignore the <pad> class
    pad_idx = class_to_idx['pad']
    model = ReferIt3DNet_transformer(args, n_classes, ignore_index=pad_idx)

    model = model.to(device)

    if args.pn_path != '':
        pn_lr = args.init_lr * 0.1
    else:
        pn_lr = args.init_lr
    param_list = [
        {'params': model.refer_encoder.parameters(), 'lr': args.init_lr * 0.1},
        {'params': model.obj_feature_mapping.parameters(), 'lr': pn_lr},
        {'params': model.box_feature_mapping.parameters(), 'lr': args.init_lr},
        {'params': model.object_encoder.parameters(), 'lr': pn_lr},
        {'params': model.obj_clf.parameters(), 'lr': pn_lr},
        {'params': model.language_target_clf.parameters(), 'lr': args.init_lr},
        {'params': model.object_language_clf.parameters(), 'lr': args.init_lr},
        {'params': model.language_encoder.parameters(), 'lr': args.init_lr * 0.1},
    ]
    if args.gat:
        param_list.append({'params': model.relation_encoder.parameters(), 'lr': args.init_lr * 0.1})
    if args.sem_encode:
        param_list.append({'params': model.obj_sem_clf.parameters(), 'lr': args.init_lr})
        param_list.append({'params': model.semantic_encoder.parameters(), 'lr': args.init_lr})
        param_list.append({'params': model.aux_feature_mapping.parameters(), 'lr': args.init_lr})
        param_list.append({'params': model.fusion_layer.parameters(), 'lr': args.init_lr})

    optimizer = optim.Adam(param_list, lr=args.init_lr)
    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [30, 40, 50, 60, 70, 80], gamma=0.65)

    start_epoch = 1
    best_test_acc = 0.0

    if args.resume_path:
        checkpoint = torch.load(args.resume_path)
        loaded_epoch = checkpoint['epoch']
        print('Loaded a model stopped at epoch: {}.'.format(loaded_epoch))
        model.load_state_dict(checkpoint['model'], strict=True)
        optimizer.load_state_dict(checkpoint['optimizer'])
        lr_scheduler.load_state_dict(checkpoint['scheduler'])
        start_epoch = loaded_epoch + 1
        best_test_acc = checkpoint['best_acc']
        print('Resuming from epoch %d, best acc %f' % (start_epoch, best_test_acc))
        del checkpoint

    if args.pn_path != '':
        pre_pn = torch.load(args.pn_path)['model']
        model.load_state_dict(pre_pn, strict=False)
        del pre_pn

    writer = SummaryWriter(args.tb_path)
    logger.info('Starting the training.')

    for epoch in range(start_epoch, args.max_epoch + 1):
        t1 = time.time()
        train_meters = single_epoch_train(model, data_loaders['train'], optimizer,
                                          device, pad_idx, args=args, tokenizer=tokenizer, epoch=epoch, logger=logger)
        t2 = time.time()
        test_meters = evaluate_on_dataset(model, data_loaders['val'], device, pad_idx, args=args,
                                          tokenizer=tokenizer)
        t3 = time.time()

        eval_acc = test_meters['test_referential_acc']
        lr_scheduler.step()

        logger.info('--- epoch %d end ---' % epoch)
        logger.info('train time %f eval_time %f  referential_acc: %f  object_cls_acc: %f  txt_cls_acc: %f' % (
            t2 - t1, t3 - t2, test_meters['test_referential_acc'], test_meters['test_object_cls_acc'],
            test_meters['test_txt_cls_acc']))
        logger.info('--------------------')
        lr = [x['lr'] for x in optimizer.param_groups]
        writer.add_scalar('train/loss', train_meters['train_total_loss'], epoch)
        writer.add_scalar('train/lr', lr[0], epoch)

        writer.add_scalar('val/referential_acc', test_meters['test_referential_acc'], epoch)
        writer.add_scalar('val/object_cls_acc', test_meters['test_object_cls_acc'], epoch)
        writer.add_scalar('val/txt_cls_acc', test_meters['test_txt_cls_acc'], epoch)

        # save
        torch.save({
            'epoch': epoch,
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'scheduler': lr_scheduler.state_dict(),
            'best_acc': max(best_test_acc, eval_acc),
        }, args.save_dir + '/ckpt_last.pth')

        if eval_acc >= best_test_acc:
            best_test_acc = eval_acc
            torch.save({
                'epoch': epoch,
                'model': model.state_dict(),
                'best_acc': best_test_acc
            }, args.save_dir + '/ckpt_best.pth')

        if epoch % 10 == 0:
            shutil.copy(args.save_dir + '/ckpt_last.pth', args.save_dir + '/ckpt_' + str(epoch) + '.pth')
